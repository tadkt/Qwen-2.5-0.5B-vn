{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLC-nPZ81Rb",
        "outputId": "ef83282f-60f5-46fe-87c5-296d96383513",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ms-swift\n",
            "  Using cached ms_swift-3.2.2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting accelerate (from ms-swift)\n",
            "  Using cached accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting addict (from ms-swift)\n",
            "  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting aiohttp (from ms-swift)\n",
            "  Using cached aiohttp-3.11.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting attrdict (from ms-swift)\n",
            "  Using cached attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting binpacking (from ms-swift)\n",
            "  Using cached binpacking-1.5.2.tar.gz (8.7 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting charset-normalizer (from ms-swift)\n",
            "  Using cached charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting cpm-kernels (from ms-swift)\n",
            "  Using cached cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dacite (from ms-swift)\n",
            "  Using cached dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting datasets>=3.0 (from ms-swift)\n",
            "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting einops (from ms-swift)\n",
            "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting fastapi (from ms-swift)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gradio>=3.40.0 (from ms-swift)\n",
            "  Using cached gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/envs/qwen/lib/python3.13/site-packages (from ms-swift) (8.6.1)\n",
            "Collecting jieba (from ms-swift)\n",
            "  Using cached jieba-0.42.1.tar.gz (19.2 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting matplotlib (from ms-swift)\n",
            "  Using cached matplotlib-3.10.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting modelscope>=1.19 (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Using cached modelscope-1.24.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting nltk (from ms-swift)\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numpy<2.0 (from ms-swift)\n",
            "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting openai (from ms-swift)\n",
            "  Downloading openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting oss2 (from ms-swift)\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pandas (from ms-swift)\n",
            "  Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting peft<0.16.0,>=0.11.0 (from ms-swift)\n",
            "  Downloading peft-0.15.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pillow (from ms-swift)\n",
            "  Downloading pillow-11.1.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting requests (from ms-swift)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting rouge (from ms-swift)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting safetensors (from ms-swift)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting sentencepiece (from ms-swift)\n",
            "  Downloading sentencepiece-0.2.0.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tensorboard (from ms-swift)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tiktoken (from ms-swift)\n",
            "  Downloading tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tqdm (from ms-swift)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting transformers<4.51,>=4.33 (from ms-swift)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting transformers-stream-generator (from ms-swift)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting trl<0.17,>=0.13 (from ms-swift)\n",
            "  Downloading trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting uvicorn (from ms-swift)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting zstandard (from ms-swift)\n",
            "  Downloading zstandard-0.23.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting filelock (from datasets>=3.0->ms-swift)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=3.0->ms-swift)\n",
            "  Downloading pyarrow-19.0.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0->ms-swift)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=3.0->ms-swift)\n",
            "  Downloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0->ms-swift)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0->ms-swift)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting huggingface-hub>=0.24.0 (from datasets>=3.0->ms-swift)\n",
            "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /opt/conda/envs/qwen/lib/python3.13/site-packages (from datasets>=3.0->ms-swift) (24.2)\n",
            "Collecting pyyaml>=5.1 (from datasets>=3.0->ms-swift)\n",
            "  Downloading PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audioop-lts<1.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading audioop_lts-0.2.1-cp313-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting ffmpy (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=3.40.0->ms-swift)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jinja2<4.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting markupsafe<4.0,>=2.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting orjson~=3.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading orjson-3.10.16-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Collecting pydantic>=2.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio>=3.40.0->ms-swift)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/envs/qwen/lib/python3.13/site-packages (from gradio>=3.40.0->ms-swift) (4.12.2)\n",
            "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio>=3.40.0->ms-swift)\n",
            "  Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting urllib3>=1.26 (from modelscope>=1.19->modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting attrs (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datasets>=3.0 (from ms-swift)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/qwen/lib/python3.13/site-packages (from modelscope[datasets]>=1.19->ms-swift) (2.9.0.post0)\n",
            "Collecting scipy (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading scipy-1.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting setuptools==69.5.1 (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting simplejson>=3.3.0 (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading simplejson-3.20.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting sortedcontainers>=1.5.9 (from modelscope[datasets]>=1.19->ms-swift)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fsspec (from gradio-client==1.8.0->gradio>=3.40.0->ms-swift)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->ms-swift)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->ms-swift)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: psutil in /opt/conda/envs/qwen/lib/python3.13/site-packages (from peft<0.16.0,>=0.11.0->ms-swift) (7.0.0)\n",
            "Collecting torch>=1.13.0 (from peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading torch-2.6.0-cp313-cp313-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->ms-swift)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->ms-swift)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers<4.51,>=4.33->ms-swift)\n",
            "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.51,>=4.33->ms-swift)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting rich (from trl<0.17,>=0.13->ms-swift)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting click>=7.0 (from uvicorn->ms-swift)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->ms-swift)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->ms-swift)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->ms-swift)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->ms-swift)\n",
            "  Downloading frozenlist-1.5.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->ms-swift)\n",
            "  Downloading multidict-6.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->ms-swift)\n",
            "  Downloading propcache-0.3.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->ms-swift)\n",
            "  Downloading yarl-1.18.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "Requirement already satisfied: six in /opt/conda/envs/qwen/lib/python3.13/site-packages (from attrdict->ms-swift) (1.17.0)\n",
            "Collecting future (from binpacking->ms-swift)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/qwen/lib/python3.13/site-packages (from importlib-metadata->ms-swift) (3.21.0)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->ms-swift)\n",
            "  Downloading contourpy-1.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->ms-swift)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->ms-swift)\n",
            "  Downloading fonttools-4.56.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->ms-swift)\n",
            "  Downloading kiwisolver-1.4.8-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->ms-swift)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting joblib (from nltk->ms-swift)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai->ms-swift)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai->ms-swift)\n",
            "  Downloading jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting sniffio (from openai->ms-swift)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->ms-swift)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pycryptodome>=3.4.7 (from oss2->ms-swift)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->ms-swift)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->ms-swift)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->ms-swift)\n",
            "  Downloading absl_py-2.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->ms-swift)\n",
            "  Downloading grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->ms-swift)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->ms-swift)\n",
            "  Downloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->ms-swift)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->ms-swift)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->ms-swift)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting cryptography>=3.0.0 (from aliyun-python-sdk-core>=2.13.12->oss2->ms-swift)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=3.40.0->ms-swift)\n",
            "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio>=3.40.0->ms-swift)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio>=3.40.0->ms-swift)\n",
            "  Downloading pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting networkx (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading triton-3.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.13.0->peft<0.16.0,>=0.11.0->ms-swift)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=3.40.0->ms-swift)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->trl<0.17,>=0.13->ms-swift)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/qwen/lib/python3.13/site-packages (from rich->trl<0.17,>=0.13->ms-swift) (2.19.1)\n",
            "Collecting cffi>=1.12 (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->ms-swift)\n",
            "  Downloading cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl<0.17,>=0.13->ms-swift)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pycparser (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->ms-swift)\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading ms_swift-3.2.2-py3-none-any.whl (593 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.5/593.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Downloading modelscope-1.24.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.0-py3-none-any.whl (410 kB)\n",
            "Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
            "Downloading pillow-11.1.0-cp313-cp313-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m198.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.16.0-py3-none-any.whl (335 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading aiohttp-3.11.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading matplotlib-3.10.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.68.2-py3-none-any.whl (606 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.2.0-py3-none-any.whl (276 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading audioop_lts-0.2.1-cp313-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading contourpy-1.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading fonttools-4.56.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.5.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (267 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "Downloading kiwisolver-1.4.8-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading multidict-6.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading orjson-3.10.16-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
            "Downloading propcache-0.3.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Downloading pyarrow-19.0.1-cp313-cp313-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Downloading pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m759.5/759.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading simplejson-3.20.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading torch-2.6.0-cp313-cp313-manylinux1_x86_64.whl (766.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m223.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m183.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m182.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m235.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m246.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m276.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m281.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m268.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading yarl-1.18.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (339 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading scipy-1.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Building wheels for collected packages: numpy, binpacking, jieba, oss2, sentencepiece, transformers-stream-generator, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for numpy: filename=numpy-1.26.4-cp313-cp313-linux_x86_64.whl size=8884465 sha256=9c9744be46b470a5ac33eae6b276dd1cddf671924591fc74ad3ed5b5e4ca5eac\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/2d/9f/b6b46373f328e2ef50388915d351ccacbedac929459b5459bf\n",
            "  Building wheel for binpacking (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for binpacking: filename=binpacking-1.5.2-py3-none-any.whl size=10133 sha256=4eaf6f56369af6e2b4cb96e4e92d3d296da9f802288290104030809a730c3385\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/af/73/39584dd41f27306b8b7cd9ac2afcd4201a24e27d9085a946ae\n",
            "  Building wheel for jieba (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=57aa2846bbca93cff4a509ab246a7d737d8c3c0876e78630091e2f4819c22dad\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/e9/51/2f0a6a9d051293af20e265d3889beae50efe2de72f8511c801\n",
            "  Building wheel for oss2 (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=124035 sha256=bfef4897faf7c423d9670920dbb636caf1a5b0666623f4c1f9fcfe8254b25e2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/a5/c4/ae8c461fdb669b208f748dd02fab0d28c052edbb3333394c8c\n",
            "  Building wheel for sentencepiece (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentencepiece: filename=sentencepiece-0.2.0-cp313-cp313-linux_x86_64.whl size=1283176 sha256=86ebbef0e3dbaa590b61cfc157fa70e37d3990ac8d70b2c4d9d14654d26a3e71\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/f0/8dbb11925fbd58340b128ae9334ae7028332d7a5751738f616\n",
            "  Building wheel for transformers-stream-generator (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12525 sha256=0aee80fc6cad1f32daa590e14b75e53220222ed72fb7c78bed4aa6766a5c2be6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6e/1a/710143d0e50e827ae5b60a47a6d43d715cf1c6e35881a05530\n",
            "  Building wheel for aliyun-python-sdk-core (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535418 sha256=54f7c9b82abafec989b8ee370e58fdc8219ccf9d52cccf8ec787b23fc01b333f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/ba/72/618a9c3b436edb6c2cc25e6e3b41eb298cf8406e234a1e0a18\n",
            "  Building wheel for crcmod (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp313-cp313-linux_x86_64.whl size=23616 sha256=65d373e00f2f5b08c6436b4a66cd35bcd69e85a3612c2ef1577340c9346e514d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/b4/7d/51d3485a8022501c7f7dba78b6c945088ff93d40142c9a96f4\n",
            "Successfully built numpy binpacking jieba oss2 sentencepiece transformers-stream-generator aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: triton, sortedcontainers, sentencepiece, pytz, pydub, nvidia-cusparselt-cu12, mpmath, jieba, crcmod, cpm-kernels, addict, zstandard, xxhash, websockets, urllib3, tzdata, tqdm, tomlkit, tensorboard-data-server, sympy, sniffio, simplejson, shellingham, setuptools, semantic-version, safetensors, ruff, rouge, regex, pyyaml, python-multipart, pyparsing, pydantic-core, pycryptodome, pycparser, pyarrow, protobuf, propcache, pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, markupsafe, markdown, kiwisolver, joblib, jmespath, jiter, idna, h11, grpcio, groovy, future, fsspec, frozenlist, fonttools, filelock, ffmpy, einops, distro, dill, dacite, cycler, click, charset-normalizer, certifi, audioop-lts, attrs, attrdict, annotated-types, aiohappyeyeballs, aiofiles, absl-py, yarl, werkzeug, uvicorn, scipy, requests, pydantic, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, markdown-it-py, jinja2, httpcore, contourpy, cffi, binpacking, anyio, aiosignal, tiktoken, tensorboard, starlette, rich, nvidia-cusolver-cu12, modelscope, matplotlib, huggingface-hub, httpx, cryptography, aiohttp, typer, torch, tokenizers, safehttpx, openai, gradio-client, fastapi, aliyun-python-sdk-core, transformers, gradio, datasets, aliyun-python-sdk-kms, accelerate, trl, transformers-stream-generator, peft, oss2, ms-swift\n",
            "Successfully installed absl-py-2.2.0 accelerate-1.5.2 addict-2.4.0 aiofiles-23.2.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 annotated-types-0.7.0 anyio-4.9.0 attrdict-2.0.1 attrs-25.3.0 audioop-lts-0.2.1 binpacking-1.5.2 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 click-8.1.8 contourpy-1.3.1 cpm-kernels-1.0.11 crcmod-1.7 cryptography-44.0.2 cycler-0.12.1 dacite-1.9.2 datasets-3.2.0 dill-0.3.8 distro-1.9.0 einops-0.8.1 fastapi-0.115.12 ffmpy-0.5.0 filelock-3.18.0 fonttools-4.56.0 frozenlist-1.5.0 fsspec-2024.9.0 future-1.0.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 grpcio-1.71.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.29.3 idna-3.10 jieba-0.42.1 jinja2-3.1.6 jiter-0.9.0 jmespath-0.10.0 joblib-1.4.2 kiwisolver-1.4.8 markdown-3.7 markdown-it-py-3.0.0 markupsafe-3.0.2 matplotlib-3.10.1 mdurl-0.1.2 modelscope-1.24.0 mpmath-1.3.0 ms-swift-3.2.2 multidict-6.2.0 multiprocess-0.70.16 networkx-3.4.2 nltk-3.9.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.68.2 orjson-3.10.16 oss2-2.19.1 pandas-2.2.3 peft-0.15.0 pillow-11.1.0 propcache-0.3.0 protobuf-6.30.1 pyarrow-19.0.1 pycparser-2.22 pycryptodome-3.22.0 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 pyparsing-3.2.3 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 rouge-1.0.1 ruff-0.11.2 safehttpx-0.1.6 safetensors-0.5.3 scipy-1.15.2 semantic-version-2.10.0 sentencepiece-0.2.0 setuptools-69.5.1 shellingham-1.5.4 simplejson-3.20.1 sniffio-1.3.1 sortedcontainers-2.4.0 starlette-0.46.1 sympy-1.13.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tiktoken-0.9.0 tokenizers-0.21.1 tomlkit-0.13.2 torch-2.6.0 tqdm-4.67.1 transformers-4.50.1 transformers-stream-generator-0.0.5 triton-3.2.0 trl-0.16.0 typer-0.15.2 tzdata-2025.2 urllib3-2.3.0 uvicorn-0.34.0 websockets-15.0.1 werkzeug-3.1.3 xxhash-3.5.0 yarl-1.18.3 zstandard-0.23.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install ms-swift -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/Qwen-2.5-0.5B-vn\n"
          ]
        }
      ],
      "source": [
        "%cd '/root/Qwen-2.5-0.5B-vn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expand_vocab  README.md  data  requirements.txt  train\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7HeCsAyT2aV",
        "outputId": "28e9da93-9ca3-4166-ccc7-586a2f285ec8",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]Downloading 'qwen_extra.tiktoken' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/HrUb_oQ6mDLZlRKeNJX5-zZhHco=.5946126a60d5fb43a9850c0051462fb5114ab1f9.incomplete'\n",
            "Downloading 'qwen.tiktoken' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/1Kd_wvgTXtvulpFNYa8YVBQZ_4Q=.4137b7938e0f678edd2094150ac8f599df033051.incomplete'\n",
            "Downloading '.gitattributes' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'tokenization_qwen.py' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/tot5DOorNnqgUrwXRlq3SWW8ciU=.2a526d66c3fc0779cb469fb9c838864ad2453d60.incomplete'\n",
            "Downloading 'config.json' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.0dbb161213629a23f0fc00ef286e6b1e366d180f.incomplete'\n",
            "Downloading 'README.md' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.7be5fc7f47d5db027d120b8024982df93db95b74.incomplete'\n",
            "\n",
            ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 17.0MB/s]\u001b[A\n",
            "\n",
            "qwen.tiktoken:   0%|                                | 0.00/2.71M [00:00<?, ?B/s]\u001b[ADownload complete. Moving file to Qwen-2.5-0.5B-VN/.gitattributes\n",
            "Fetching 10 files:  10%|██▌                      | 1/10 [00:00<00:02,  3.35it/s]\n",
            "\n",
            "qwen_extra.tiktoken:   0%|                           | 0.00/254k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenization_qwen.py: 100%|████████████████| 9.62k/9.62k [00:00<00:00, 36.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/tokenization_qwen.py\n",
            "Downloading 'model.safetensors' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 5.25MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/config.json\n",
            "Downloading 'generation_config.json' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.dfc11073787daf1b0f9c0f1499487ab5f4c93738.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "README.md: 100%|██████████████████████████████| 24.0/24.0 [00:00<00:00, 287kB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/README.md\n",
            "Downloading 'tokenizer.json' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.443909a61d429dff23010e5bddd28ff530edda00.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.28MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/generation_config.json\n",
            "Fetching 10 files:  40%|██████████               | 4/10 [00:00<00:00,  9.11it/s]\n",
            "\n",
            "qwen_extra.tiktoken: 100%|███████████████████| 254k/254k [00:00<00:00, 1.25MB/s]\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/qwen_extra.tiktoken\n",
            "\n",
            "\n",
            "model.safetensors:   0%|                             | 0.00/988M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json:   0%|                               | 0.00/7.03M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   1%|▏                   | 10.5M/988M [00:00<00:26, 36.6MB/s]\u001b[A\u001b[A\n",
            "qwen.tiktoken: 100%|███████████████████████| 2.71M/2.71M [00:00<00:00, 5.26MB/s]\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/qwen.tiktoken\n",
            "\n",
            "\n",
            "model.safetensors:   2%|▍                   | 21.0M/988M [00:00<00:20, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   3%|▋                   | 31.5M/988M [00:00<00:16, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 11.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/tokenizer.json\n",
            "\n",
            "\n",
            "model.safetensors:   5%|█                   | 52.4M/988M [00:00<00:13, 70.6MB/s]\u001b[A\u001b[ADownloading 'tokenizer_config.json' to 'Qwen-2.5-0.5B-VN/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.9b9a8b74f6a580703f21fb095fcb78acd7cca173.incomplete'\n",
            "\n",
            "\n",
            "model.safetensors:   6%|█▎                  | 62.9M/988M [00:00<00:13, 70.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   7%|█▍                  | 73.4M/988M [00:01<00:13, 69.7MB/s]\u001b[A\u001b[A\n",
            "tokenizer_config.json: 100%|███████████████| 7.41k/7.41k [00:00<00:00, 30.3MB/s]\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/tokenizer_config.json\n",
            "\n",
            "\n",
            "model.safetensors:   8%|█▋                  | 83.9M/988M [00:01<00:13, 69.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  10%|█▉                  | 94.4M/988M [00:01<00:12, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  11%|██▏                  | 105M/988M [00:01<00:12, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  12%|██▍                  | 115M/988M [00:01<00:12, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  13%|██▋                  | 126M/988M [00:01<00:12, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  14%|██▉                  | 136M/988M [00:02<00:12, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  15%|███                  | 147M/988M [00:02<00:12, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  16%|███▎                 | 157M/988M [00:02<00:12, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17%|███▌                 | 168M/988M [00:02<00:11, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  18%|███▊                 | 178M/988M [00:02<00:12, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  19%|████                 | 189M/988M [00:02<00:13, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  20%|████▏                | 199M/988M [00:03<00:12, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21%|████▍                | 210M/988M [00:03<00:12, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  22%|████▋                | 220M/988M [00:03<00:11, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23%|████▉                | 231M/988M [00:03<00:11, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  24%|█████▏               | 241M/988M [00:03<00:11, 66.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  25%|█████▎               | 252M/988M [00:03<00:11, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  27%|█████▌               | 262M/988M [00:04<00:11, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28%|█████▊               | 273M/988M [00:04<00:10, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  29%|██████               | 283M/988M [00:04<00:10, 67.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  30%|██████▏              | 294M/988M [00:04<00:10, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  31%|██████▍              | 304M/988M [00:04<00:10, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  32%|██████▋              | 315M/988M [00:04<00:09, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  33%|██████▉              | 325M/988M [00:04<00:09, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34%|███████▏             | 336M/988M [00:05<00:09, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  35%|███████▎             | 346M/988M [00:05<00:09, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  36%|███████▌             | 357M/988M [00:05<00:09, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  37%|███████▊             | 367M/988M [00:05<00:09, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  38%|████████             | 377M/988M [00:05<00:09, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  39%|████████▏            | 388M/988M [00:05<00:08, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  40%|████████▍            | 398M/988M [00:06<00:08, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  41%|████████▋            | 409M/988M [00:06<00:08, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  42%|████████▉            | 419M/988M [00:06<00:08, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  44%|█████████▏           | 430M/988M [00:06<00:08, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  45%|█████████▎           | 440M/988M [00:06<00:08, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46%|█████████▌           | 451M/988M [00:06<00:08, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47%|█████████▊           | 461M/988M [00:06<00:07, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  48%|██████████           | 472M/988M [00:07<00:07, 67.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49%|██████████▎          | 482M/988M [00:07<00:07, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  50%|██████████▍          | 493M/988M [00:07<00:07, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51%|██████████▋          | 503M/988M [00:07<00:07, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  52%|██████████▉          | 514M/988M [00:07<00:06, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53%|███████████▏         | 524M/988M [00:07<00:06, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  54%|███████████▎         | 535M/988M [00:08<00:06, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  55%|███████████▌         | 545M/988M [00:08<00:06, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56%|███████████▊         | 556M/988M [00:08<00:06, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  57%|████████████         | 566M/988M [00:08<00:06, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  58%|████████████▎        | 577M/988M [00:08<00:06, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  59%|████████████▍        | 587M/988M [00:08<00:05, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  60%|████████████▋        | 598M/988M [00:08<00:05, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  62%|████████████▉        | 608M/988M [00:09<00:05, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  63%|█████████████▏       | 619M/988M [00:09<00:05, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  64%|█████████████▎       | 629M/988M [00:09<00:05, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  65%|█████████████▌       | 640M/988M [00:09<00:05, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  66%|█████████████▊       | 650M/988M [00:09<00:04, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  67%|██████████████       | 661M/988M [00:09<00:04, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  68%|██████████████▎      | 671M/988M [00:10<00:04, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69%|██████████████▍      | 682M/988M [00:10<00:04, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  70%|██████████████▋      | 692M/988M [00:10<00:04, 67.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  71%|██████████████▉      | 703M/988M [00:10<00:04, 71.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  72%|███████████████▏     | 713M/988M [00:10<00:03, 70.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  73%|███████████████▍     | 724M/988M [00:10<00:03, 67.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  74%|███████████████▌     | 734M/988M [00:10<00:03, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  75%|███████████████▊     | 744M/988M [00:11<00:04, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  76%|████████████████     | 755M/988M [00:11<00:03, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  77%|████████████████▎    | 765M/988M [00:11<00:03, 71.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79%|████████████████▍    | 776M/988M [00:11<00:03, 69.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80%|████████████████▋    | 786M/988M [00:11<00:02, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81%|████████████████▉    | 797M/988M [00:11<00:02, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82%|█████████████████▏   | 807M/988M [00:12<00:02, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83%|█████████████████▍   | 818M/988M [00:12<00:02, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  84%|█████████████████▌   | 828M/988M [00:12<00:02, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85%|█████████████████▊   | 839M/988M [00:12<00:02, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86%|██████████████████   | 849M/988M [00:12<00:02, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87%|██████████████████▎  | 860M/988M [00:12<00:01, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88%|██████████████████▍  | 870M/988M [00:13<00:01, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89%|██████████████████▋  | 881M/988M [00:13<00:01, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  90%|██████████████████▉  | 891M/988M [00:13<00:01, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91%|███████████████████▏ | 902M/988M [00:13<00:01, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92%|███████████████████▍ | 912M/988M [00:13<00:01, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93%|███████████████████▌ | 923M/988M [00:13<00:00, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94%|███████████████████▊ | 933M/988M [00:13<00:00, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96%|████████████████████ | 944M/988M [00:14<00:00, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97%|████████████████████▎| 954M/988M [00:14<00:00, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  98%|████████████████████▌| 965M/988M [00:14<00:00, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  99%|████████████████████▋| 975M/988M [00:14<00:00, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100%|█████████████████████| 988M/988M [00:14<00:00, 67.0MB/s]\u001b[A\u001b[A\n",
            "Download complete. Moving file to Qwen-2.5-0.5B-VN/model.safetensors\n",
            "Fetching 10 files: 100%|████████████████████████| 10/10 [00:15<00:00,  1.53s/it]\n",
            "/root/Qwen-2.5-0.5B-vn/Qwen-2.5-0.5B-VN\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download tadkt/Qwen-2.5-0.5B-VN --local-dir ./Qwen-2.5-0.5B-VN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNFL7bkFA_vt",
        "outputId": "ca25f767-e5bc-482a-9389-0c04bdb3db9f",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 72.2M  100 72.2M    0     0  25.4M      0  0:00:02  0:00:02 --:--:-- 34.2M\n",
            "Archive:  vnmese-nlp.zip\n",
            "  inflating: data/proccessed_data.jsonl  \n",
            "  inflating: data/proccessed_data_1.jsonl  \n",
            "  inflating: data/proccessed_data_2.jsonl  \n",
            "  inflating: data/proccessed_data_3.jsonl  \n",
            "  inflating: data/proccessed_data_4.jsonl  \n"
          ]
        }
      ],
      "source": [
        "!curl -L -o ./vnmese-nlp.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/tadktthai/vnmese-nlp\n",
        "!unzip -o vnmese-nlp.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "996sVyJC5NPf"
      },
      "source": [
        "## Edit the library function register to add additional vocab\n",
        "- In \"/usr/local/lib/python3.11/dist-packages/swift/llm/model/register.py\", line 183:\n",
        "```\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, extra_vocab_file='/content/Qwen-2.5-0.5B-VN/qwen_extra.tiktoken')\n",
        "```\n",
        "line 211:\n",
        "```\n",
        "# Resize embedding layer if tokenizer vocab size exceeds model embedding size\n",
        "        if model is not None and hasattr(model, 'resize_token_embeddings'):\n",
        "            new_vocab_size = len(tokenizer)\n",
        "            original_embedding_size = model.get_input_embeddings().weight.size(0)\n",
        "            if new_vocab_size > original_embedding_size:\n",
        "                logger.info(f\"Resizing model embedding layer from {original_embedding_size} to {new_vocab_size}\")\n",
        "                model.resize_token_embeddings(new_vocab_size)\n",
        "                # Initialize new embeddings with the mean of original embeddings\n",
        "                with torch.no_grad():\n",
        "                    original_embeddings = model.get_input_embeddings().weight[:original_embedding_size]\n",
        "                    mean_embedding = original_embeddings.mean(dim=0)\n",
        "                    model.get_input_embeddings().weight[original_embedding_size:] = mean_embedding\n",
        "                logger.info(\"New embeddings initialized with mean of original embeddings. Fine-tuning recommended.\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfo8AM8g5OC_",
        "outputId": "b583b9c2-be29-48c8-bfcc-678fd143b8d7",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/model/register.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/model/register.py\n",
        "# Copyright (c) Alibaba, Inc. and its affiliates.\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "from contextlib import nullcontext\n",
        "from copy import deepcopy\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (AutoConfig, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, GenerationConfig, PretrainedConfig, PreTrainedModel, PreTrainedTokenizerBase)\n",
        "from transformers.integrations import is_deepspeed_zero3_enabled\n",
        "from transformers.utils import (is_torch_bf16_gpu_available, is_torch_cuda_available, is_torch_mps_available,\n",
        "                                is_torch_npu_available, strtobool)\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "from swift.utils import get_dist_setting, get_logger, is_mp, is_unsloth_available, patch_getattr, use_torchacc\n",
        "from .constant import ModelType\n",
        "from .patcher import (patch_automodel, patch_automodel_for_sequence_classification, patch_get_dynamic_module,\n",
        "                      patch_mp_ddp)\n",
        "from .utils import AttnImpl, HfConfigFactory, ModelInfo, safe_snapshot_download\n",
        "\n",
        "GetModelTokenizerFunction = Callable[..., Tuple[Optional[PreTrainedModel], PreTrainedTokenizerBase]]\n",
        "logger = get_logger()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Model:\n",
        "    ms_model_id: Optional[str] = None\n",
        "    hf_model_id: Optional[str] = None\n",
        "    model_path: Optional[str] = None\n",
        "\n",
        "    ms_revision: Optional[str] = None\n",
        "    hf_revision: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelGroup:\n",
        "    models: List[Model]\n",
        "\n",
        "    # Higher priority. If set to None, the attributes of the ModelMeta will be used.\n",
        "    ignore_patterns: Optional[List[str]] = None\n",
        "    requires: Optional[List[str]] = None\n",
        "    tags: List[str] = field(default_factory=list)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not isinstance(self.models, (tuple, list)):\n",
        "            self.models = [self.models]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelMeta:\n",
        "    model_type: Optional[str]\n",
        "    # Used to list the model_ids from modelscope/huggingface,\n",
        "    # which participate in the automatic inference of the model_type.\n",
        "    model_groups: List[ModelGroup]\n",
        "    template: Optional[str]\n",
        "    get_function: GetModelTokenizerFunction\n",
        "\n",
        "    model_arch: Optional[str] = None\n",
        "    architectures: List[str] = field(default_factory=list)\n",
        "    # Additional files that need to be saved for full parameter training/merge-lora.\n",
        "    additional_saved_files: List[str] = field(default_factory=list)\n",
        "    torch_dtype: Optional[torch.dtype] = None\n",
        "\n",
        "    is_multimodal: bool = False\n",
        "    is_reward: bool = False\n",
        "    task_type: Optional[str] = None\n",
        "\n",
        "    # File patterns to ignore when downloading the model.\n",
        "    ignore_patterns: List[str] = field(default_factory=list)\n",
        "    # Usually specifies the version limits of transformers.\n",
        "    requires: List[str] = field(default_factory=list)\n",
        "    tags: List[str] = field(default_factory=list)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.template is None:\n",
        "            self.template = 'dummy'\n",
        "        if not isinstance(self.model_groups, (list, tuple)):\n",
        "            self.model_groups = [self.model_groups]\n",
        "\n",
        "    def get_matched_model_group(self, model_name: str) -> Optional[ModelGroup]:\n",
        "        for model_group in self.model_groups:\n",
        "            for model in model_group.models:\n",
        "                for key in ['ms_model_id', 'hf_model_id', 'model_path']:\n",
        "                    value = getattr(model, key)\n",
        "\n",
        "                    if isinstance(value, str) and model_name == value.rsplit('/', 1)[-1].lower():\n",
        "                        return model_group\n",
        "\n",
        "    def check_requires(self, model_info=None):\n",
        "        extra_requires = []\n",
        "        if model_info and model_info.quant_method:\n",
        "            mapping = {'bnb': ['bitsandbytes'], 'awq': ['autoawq'], 'gptq': ['auto_gptq'], 'aqlm': ['aqlm']}\n",
        "            extra_requires += mapping.get(model_info.quant_method, [])\n",
        "        requires = []\n",
        "        for require in self.requires + extra_requires:\n",
        "            try:\n",
        "                require_version(require)\n",
        "            except ImportError:\n",
        "                requires.append(f'\"{require}\"')\n",
        "        if requires:\n",
        "            requires = ' '.join(requires)\n",
        "            logger.warning(f'Please install the package: `pip install {requires} -U`.')\n",
        "\n",
        "\n",
        "MODEL_MAPPING: Dict[str, ModelMeta] = {}\n",
        "\n",
        "\n",
        "def register_model(model_meta: ModelMeta, *, exist_ok: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    model_type: The unique ID for the model type. Models with the same model_type share\n",
        "        the same architectures, template, get_function, etc.\n",
        "    \"\"\"\n",
        "    model_type = model_meta.model_type\n",
        "    if not exist_ok and model_type in MODEL_MAPPING:\n",
        "        raise ValueError(f'The `{model_type}` has already been registered in the MODEL_MAPPING.')\n",
        "    from .constant import MLLMModelType, RMModelType\n",
        "    if model_type in MLLMModelType.__dict__:\n",
        "        model_meta.is_multimodal = True\n",
        "    if model_type in RMModelType.__dict__:\n",
        "        model_meta.is_reward = True\n",
        "    MODEL_MAPPING[model_type] = model_meta\n",
        "\n",
        "\n",
        "def load_by_unsloth(args):\n",
        "    \"\"\"Load model by unsloth\"\"\"\n",
        "    assert is_unsloth_available(), 'please install unsloth if using `use_unsloth=True`: `pip install unsloth`'\n",
        "    os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
        "    os.environ['UNSLOTH_DISABLE_STATISTICS'] = '1'\n",
        "    model_info = args.model_info\n",
        "    model_meta = args.model_meta\n",
        "    if model_meta.is_multimodal:\n",
        "        from unsloth import FastVisionModel as UnslothModel\n",
        "    else:\n",
        "        from unsloth import FastLanguageModel as UnslothModel\n",
        "    model, processor = UnslothModel.from_pretrained(\n",
        "        model_name=args.adapters and args.adapters[0] or args.model_dir,\n",
        "        dtype=args.torch_dtype,\n",
        "        max_seq_length=args.max_length,\n",
        "        load_in_4bit=args.quant_bits == 4,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    if isinstance(model, PeftModel):\n",
        "        base_model = model.model\n",
        "    else:\n",
        "        base_model = model\n",
        "    base_model.model_dir = args.model_dir\n",
        "    base_model.model_info = model_info\n",
        "    base_model.model_meta = model_meta\n",
        "    processor.model_info = model_info\n",
        "    processor.model_meta = model_meta\n",
        "    return model, processor\n",
        "\n",
        "\n",
        "def get_model_tokenizer_from_local(model_dir: str,\n",
        "                                   model_info: ModelInfo,\n",
        "                                   model_kwargs: Dict[str, Any],\n",
        "                                   load_model: bool = True,\n",
        "                                   *,\n",
        "                                   tokenizer=None,\n",
        "                                   model_config=None,\n",
        "                                   automodel_class=None,\n",
        "                                   **kwargs):\n",
        "    \"\"\"Load the model and tokenizer from the local model_dir.\"\"\"\n",
        "    if model_config is None:\n",
        "        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
        "    # fix prediction_step (internvl2, ovis, ...)\n",
        "    if not hasattr(model_config, 'keys_to_ignore_at_inference'):\n",
        "        model_config.keys_to_ignore_at_inference = []\n",
        "    if 'past_key_values' not in model_config.keys_to_ignore_at_inference:\n",
        "        model_config.keys_to_ignore_at_inference.append('past_key_values')\n",
        "    model_info.config = model_config\n",
        "\n",
        "    torch_dtype = model_info.torch_dtype\n",
        "    model_config.torch_dtype = torch_dtype\n",
        "    HfConfigFactory.compat_zero3(model_config)\n",
        "    rope_scaling = kwargs.get('rope_scaling')\n",
        "    if rope_scaling is not None:\n",
        "        HfConfigFactory.set_config_attr(model_config, 'rope_scaling', rope_scaling)\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, extra_vocab_file='./Qwen-2.5-0.5B-VN/qwen_extra.tiktoken')\n",
        "\n",
        "    num_labels = model_info.num_labels or getattr(model_config, 'num_labels', None)\n",
        "    if num_labels and model_info.task_type != 'causal_lm':\n",
        "        model_info.num_labels = num_labels\n",
        "        model_config.num_labels = num_labels\n",
        "\n",
        "    model = None\n",
        "    if load_model:\n",
        "        logger.info(f'model_kwargs: {model_kwargs}')\n",
        "        # fix seq_cls\n",
        "        if model_info.task_type == 'seq_cls' and automodel_class is None:\n",
        "            try:\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    model_dir, config=model_config, torch_dtype=torch_dtype, trust_remote_code=True, **model_kwargs)\n",
        "            except ValueError:\n",
        "                model = None\n",
        "\n",
        "        if model_info.task_type == 'embedding' and automodel_class is None:\n",
        "            try:\n",
        "                model = AutoModel.from_pretrained(\n",
        "                    model_dir, config=model_config, torch_dtype=torch_dtype, trust_remote_code=True, **model_kwargs)\n",
        "                from swift.llm.model.patcher import patch_output_normalizer\n",
        "                patch_output_normalizer(model)\n",
        "            except ValueError:\n",
        "                model = None\n",
        "\n",
        "        automodel_class = automodel_class or AutoModelForCausalLM\n",
        "        if model is None:\n",
        "            if model_info.task_type == 'seq_cls':\n",
        "                context = partial(patch_automodel_for_sequence_classification, model_meta=kwargs['model_meta'])\n",
        "            else:\n",
        "                context = partial(patch_automodel, automodel_class=automodel_class, model_info=model_info)\n",
        "            with context():\n",
        "                model = automodel_class.from_pretrained(\n",
        "                    model_dir, config=model_config, torch_dtype=torch_dtype, trust_remote_code=True, **model_kwargs)\n",
        "\n",
        "        # Resize embedding layer if tokenizer vocab size exceeds model embedding size\n",
        "        if model is not None and hasattr(model, 'resize_token_embeddings'):\n",
        "            new_vocab_size = len(tokenizer)\n",
        "            original_embedding_size = model.get_input_embeddings().weight.size(0)\n",
        "            if new_vocab_size > original_embedding_size:\n",
        "                logger.info(f\"Resizing model embedding layer from {original_embedding_size} to {new_vocab_size}\")\n",
        "                model.resize_token_embeddings(new_vocab_size)\n",
        "                # Initialize new embeddings with the mean of original embeddings\n",
        "                with torch.no_grad():\n",
        "                    original_embeddings = model.get_input_embeddings().weight[:original_embedding_size]\n",
        "                    mean_embedding = original_embeddings.mean(dim=0)\n",
        "                    model.get_input_embeddings().weight[original_embedding_size:] = mean_embedding\n",
        "                logger.info(\"New embeddings initialized with mean of original embeddings. Fine-tuning recommended.\")\n",
        "        # https://github.com/huggingface/transformers/issues/24737\n",
        "        has_remote_code = hasattr(model_config, 'auto_map') and automodel_class.__name__ in model_config.auto_map\n",
        "        if has_remote_code and model._auto_class is None:\n",
        "            model._auto_class = automodel_class.__name__\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_model_tokenizer_with_flash_attn(model_dir: str,\n",
        "                                        model_info: ModelInfo,\n",
        "                                        model_kwargs: Dict[str, Any],\n",
        "                                        load_model: bool = True,\n",
        "                                        **kwargs):\n",
        "    model_config = kwargs.get('model_config')\n",
        "    if model_config is None:\n",
        "        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
        "    AttnImpl.update_attn_impl(model_config, kwargs.get('attn_impl'), kwargs.get('attn_impl_keys'))\n",
        "    kwargs['model_config'] = model_config\n",
        "    return get_model_tokenizer_from_local(model_dir, model_info, model_kwargs, load_model, **kwargs)\n",
        "\n",
        "\n",
        "def get_model_tokenizer_multimodal(model_dir: str, *args, **kwargs):\n",
        "    from transformers import AutoProcessor\n",
        "    processor = AutoProcessor.from_pretrained(model_dir, trust_remote_code=True)\n",
        "    kwargs['tokenizer'] = processor.tokenizer\n",
        "    model, _ = get_model_tokenizer_with_flash_attn(model_dir, *args, **kwargs)\n",
        "    return model, processor\n",
        "\n",
        "\n",
        "def get_model_tokenizer_reward_model(model_dir, *args, **kwargs):\n",
        "    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
        "    if 'AutoModel' in (getattr(model_config, 'auto_map', None) or {}):\n",
        "        kwargs['automodel_class'] = AutoModel\n",
        "    return get_model_tokenizer_with_flash_attn(model_dir, *args, **kwargs)\n",
        "\n",
        "\n",
        "def fix_do_sample_warning(generation_config: GenerationConfig) -> None:\n",
        "    # Use the default values of temperature/top_p/top_k in generation_config.\n",
        "    if generation_config.temperature == 0:\n",
        "        generation_config.do_sample = False\n",
        "    if generation_config.do_sample is False:\n",
        "        generation_config.temperature = 1.\n",
        "        generation_config.top_p = 1.\n",
        "        generation_config.top_k = 50\n",
        "\n",
        "\n",
        "def get_default_device_map():\n",
        "    if is_deepspeed_zero3_enabled() or os.environ.get('ACCELERATE_USE_FSDP', 'False') == 'true':\n",
        "        return None\n",
        "    local_rank = get_dist_setting()[1]\n",
        "    if local_rank == -1:\n",
        "        local_rank = 0\n",
        "    if is_torch_npu_available():\n",
        "        return f'npu:{local_rank}'\n",
        "    elif is_torch_mps_available():\n",
        "        return f'mps:{local_rank}'\n",
        "    elif is_torch_cuda_available():\n",
        "        if is_mp():\n",
        "            return 'auto'\n",
        "        else:\n",
        "            return f'cuda:{local_rank}'\n",
        "    else:\n",
        "        return 'cpu'\n",
        "\n",
        "\n",
        "def get_default_torch_dtype(torch_dtype: Optional[torch.dtype]):\n",
        "    # torch_dtype: torch_dtype in config.json\n",
        "    if torch_dtype is not None:\n",
        "        return torch_dtype\n",
        "\n",
        "    try:\n",
        "        is_bf16_available = is_torch_bf16_gpu_available() or (is_torch_npu_available()\n",
        "                                                              and torch.npu.is_bf16_supported())\n",
        "    except:  # noqa\n",
        "        is_bf16_available = False\n",
        "\n",
        "    if is_torch_cuda_available() or is_torch_npu_available():\n",
        "        if is_bf16_available:\n",
        "            return torch.bfloat16\n",
        "        else:\n",
        "            return torch.float16\n",
        "    else:\n",
        "        # cpu\n",
        "        return torch.float32\n",
        "\n",
        "\n",
        "def get_model_name(model_id_or_path: str) -> Optional[str]:\n",
        "    assert isinstance(model_id_or_path, str), f'model_id_or_path: {model_id_or_path}'\n",
        "    # compat hf hub\n",
        "    model_id_or_path = model_id_or_path.rstrip('/')\n",
        "    match_ = re.search('/models--.+?--(.+?)/snapshots/', model_id_or_path)\n",
        "    if match_ is not None:\n",
        "        return match_.group(1)\n",
        "\n",
        "    model_name = model_id_or_path.rsplit('/', 1)[-1]\n",
        "    if platform.system().lower() == 'windows':\n",
        "        model_name = model_name.rsplit('\\\\', 1)[-1]\n",
        "    # compat modelscope snapshot_download\n",
        "    model_name = model_name.replace('___', '.')\n",
        "    return model_name\n",
        "\n",
        "\n",
        "def get_all_models() -> List[str]:\n",
        "    use_hf = strtobool(os.environ.get('USE_HF', 'False'))\n",
        "    models = []\n",
        "    for model_type in ModelType.get_model_name_list():\n",
        "        model_meta = MODEL_MAPPING.get(model_type)\n",
        "        if model_meta:\n",
        "            for group in model_meta.model_groups:\n",
        "                for model in group.models:\n",
        "                    if use_hf:\n",
        "                        if model.hf_model_id:\n",
        "                            models.append(model.hf_model_id)\n",
        "                    else:\n",
        "                        if model.ms_model_id:\n",
        "                            models.append(model.ms_model_id)\n",
        "    return models\n",
        "\n",
        "\n",
        "def get_matched_model_meta(model_id_or_path: str) -> Optional[ModelMeta]:\n",
        "    model_name = get_model_name(model_id_or_path).lower()\n",
        "    for model_type, model_meta in MODEL_MAPPING.items():\n",
        "        model_group = model_meta.get_matched_model_group(model_name)\n",
        "        if model_group is not None:\n",
        "            model_meta = deepcopy(model_meta)\n",
        "            for k, v in asdict(model_group).items():\n",
        "                if v is not None and k in model_meta.__dict__:\n",
        "                    setattr(model_meta, k, v)\n",
        "            return model_meta\n",
        "\n",
        "\n",
        "def _get_arch_mapping():\n",
        "    res = {}\n",
        "    for model_type, model_meta in MODEL_MAPPING.items():\n",
        "        architectures = model_meta.architectures\n",
        "        if not architectures:\n",
        "            architectures.append('null')\n",
        "        for arch in architectures:\n",
        "            if arch not in res:\n",
        "                res[arch] = []\n",
        "            res[arch].append(model_type)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_matched_model_types(architectures: Optional[List[str]]) -> List[str]:\n",
        "    \"\"\"Get possible model_type.\"\"\"\n",
        "    architectures = architectures or ['null']\n",
        "    if architectures:\n",
        "        architectures = architectures[0]\n",
        "    arch_mapping = _get_arch_mapping()\n",
        "    return arch_mapping.get(architectures) or []\n",
        "\n",
        "\n",
        "def _read_args_json_model_type(model_dir):\n",
        "    if not os.path.exists(os.path.join(model_dir, 'args.json')):\n",
        "        return\n",
        "    from swift.llm import BaseArguments\n",
        "    args = BaseArguments.from_pretrained(model_dir)\n",
        "    return args.model_type\n",
        "\n",
        "\n",
        "def _get_model_info(model_dir: str, model_type: Optional[str], quantization_config) -> ModelInfo:\n",
        "    config_dict = PretrainedConfig.get_config_dict(model_dir)[0]\n",
        "    if quantization_config is not None:\n",
        "        config_dict['quantization_config'] = quantization_config\n",
        "    quant_info = HfConfigFactory.get_quant_info(config_dict) or {}\n",
        "    torch_dtype = HfConfigFactory.get_torch_dtype(config_dict, quant_info)\n",
        "    max_model_len = HfConfigFactory.get_max_model_len(config_dict)\n",
        "    rope_scaling = HfConfigFactory.get_config_attr(config_dict, 'rope_scaling')\n",
        "\n",
        "    if model_type is None:\n",
        "        model_type = _read_args_json_model_type(model_dir)\n",
        "    if model_type is None:\n",
        "        architectures = HfConfigFactory.get_config_attr(config_dict, 'architectures')\n",
        "        model_types = get_matched_model_types(architectures)\n",
        "        if len(model_types) > 1:\n",
        "            raise ValueError('Please explicitly pass the model_type. For reference, '\n",
        "                             f'the available model_types: {model_types}.')\n",
        "        elif len(model_types) == 1:\n",
        "            model_type = model_types[0]\n",
        "    elif model_type not in MODEL_MAPPING:\n",
        "        raise ValueError(f\"model_type: '{model_type}' not in {list(MODEL_MAPPING.keys())}\")\n",
        "\n",
        "    res = ModelInfo(\n",
        "        model_type,\n",
        "        model_dir,\n",
        "        torch_dtype,\n",
        "        max_model_len,\n",
        "        quant_info.get('quant_method'),\n",
        "        quant_info.get('quant_bits'),\n",
        "        rope_scaling=rope_scaling)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_model_info_meta(\n",
        "        model_id_or_path: str,\n",
        "        torch_dtype: Optional[torch.dtype] = None,\n",
        "        *,\n",
        "        # hub\n",
        "        use_hf: Optional[bool] = None,\n",
        "        hub_token: Optional[str] = None,\n",
        "        revision: Optional[str] = None,\n",
        "        download_model: bool = False,\n",
        "        # model kwargs\n",
        "        model_type: Optional[str] = None,\n",
        "        quantization_config=None,\n",
        "        task_type=None,\n",
        "        num_labels=None,\n",
        "        **kwargs) -> Tuple[ModelInfo, ModelMeta]:\n",
        "    model_meta = get_matched_model_meta(model_id_or_path)\n",
        "    model_dir = safe_snapshot_download(\n",
        "        model_id_or_path,\n",
        "        revision=revision,\n",
        "        download_model=download_model,\n",
        "        use_hf=use_hf,\n",
        "        ignore_patterns=getattr(model_meta, 'ignore_patterns', None),\n",
        "        hub_token=hub_token)\n",
        "\n",
        "    model_type = model_type or getattr(model_meta, 'model_type', None)\n",
        "    model_info = _get_model_info(model_dir, model_type, quantization_config=quantization_config)\n",
        "    if model_type is None and model_info.model_type is not None:\n",
        "        model_type = model_info.model_type\n",
        "        logger.info(f'Setting model_type: {model_type}')\n",
        "    if model_meta is None and model_type is not None:\n",
        "        model_meta = MODEL_MAPPING[model_type]\n",
        "    if model_meta is None:\n",
        "        model_meta = ModelMeta(None, [], 'dummy', get_model_tokenizer_from_local, model_arch=None)\n",
        "        logger.info(f'Temporarily create model_meta: {model_meta}')\n",
        "\n",
        "    if torch_dtype is None:\n",
        "        torch_dtype = model_meta.torch_dtype or get_default_torch_dtype(model_info.torch_dtype)\n",
        "        logger.info(f'Setting torch_dtype: {torch_dtype}')\n",
        "    model_info.torch_dtype = torch_dtype\n",
        "    if task_type is None:\n",
        "        if model_meta.is_reward:\n",
        "            num_labels = 1\n",
        "        if num_labels is None:\n",
        "            task_type = 'causal_lm'\n",
        "        else:\n",
        "            task_type = 'seq_cls'\n",
        "        if task_type == 'seq_cls':\n",
        "            assert num_labels is not None, 'Please pass the parameter `num_labels`.'\n",
        "        if model_meta.task_type is not None:\n",
        "            task_type = model_meta.task_type\n",
        "    model_info.task_type = task_type\n",
        "    model_info.num_labels = num_labels\n",
        "\n",
        "    model_meta.check_requires(model_info)\n",
        "    return model_info, model_meta\n",
        "\n",
        "\n",
        "def get_model_tokenizer(\n",
        "        model_id_or_path: str,\n",
        "        torch_dtype: Optional[torch.dtype] = None,\n",
        "        device_map: Union[str, Dict[str, Any], None] = None,\n",
        "        *,\n",
        "        load_model: bool = True,\n",
        "        # hub\n",
        "        use_hf: Optional[bool] = None,\n",
        "        hub_token: Optional[str] = None,\n",
        "        revision: Optional[str] = None,\n",
        "        download_model: Optional[bool] = None,\n",
        "        # model kwargs\n",
        "        model_type: Optional[str] = None,\n",
        "        quantization_config=None,\n",
        "        max_memory: Union[str, Dict[str, Any]] = None,\n",
        "        attn_impl: Literal['flash_attn', 'sdpa', 'eager', None] = None,\n",
        "        rope_scaling: Optional[Dict[str, Any]] = None,\n",
        "        automodel_class=None,\n",
        "        task_type: Literal['causal_lm', 'seq_cls'] = None,\n",
        "        num_labels: Optional[int] = None,\n",
        "        model_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs) -> Tuple[Optional[PreTrainedModel], PreTrainedTokenizerBase]:\n",
        "    \"\"\"\n",
        "    model_id_or_path: The path to the model or the model_id from modelscope/huggingface (controlled by `use_hf`).\n",
        "    torch_dtype: If you pass `None`, it will retrieve the torch_dtype from the config.json file.\n",
        "    model_kwargs: Passed to `automodel_class.from_pretrained`.\n",
        "    load_model: Whether to load the model. If set to False, the model will return `None`.\n",
        "    use_hf: Indicates whether the model download hub is modelscope or huggingface.\n",
        "    model_type: If it is not possible to uniquely determine the model_type from the architecture in config.json,\n",
        "        it needs to be provided.\n",
        "    attn_impl: If set to 'flash_attn': It will automatically convert names based on the model.\n",
        "        If set to None : It will be automatically selected between sdpa and eager.\n",
        "    download_model: Whether to download the model weights. If `None`, it will be selected based on load_model.\n",
        "    \"\"\"\n",
        "    patch_mp_ddp()\n",
        "    if model_kwargs is None:\n",
        "        model_kwargs = {}\n",
        "    if download_model is None:\n",
        "        download_model = load_model\n",
        "\n",
        "    model_info, model_meta = get_model_info_meta(\n",
        "        model_id_or_path,\n",
        "        torch_dtype,\n",
        "        use_hf=use_hf,\n",
        "        hub_token=hub_token,\n",
        "        revision=revision,\n",
        "        download_model=download_model,\n",
        "        model_type=model_type,\n",
        "        quantization_config=quantization_config,\n",
        "        task_type=task_type,\n",
        "        num_labels=num_labels)\n",
        "\n",
        "    if not use_torchacc() and device_map is None:\n",
        "        device_map = get_default_device_map()\n",
        "    model_kwargs['device_map'] = device_map\n",
        "    if quantization_config:\n",
        "        model_kwargs['quantization_config'] = quantization_config\n",
        "    if max_memory:\n",
        "        model_kwargs['max_memory'] = max_memory\n",
        "    model_dir = model_info.model_dir\n",
        "    get_function = model_meta.get_function\n",
        "    kwargs['automodel_class'] = automodel_class\n",
        "    kwargs['attn_impl'] = attn_impl\n",
        "    kwargs['rope_scaling'] = rope_scaling\n",
        "    kwargs['model_meta'] = model_meta\n",
        "    with patch_get_dynamic_module():\n",
        "        model, processor = get_function(model_dir, model_info, model_kwargs, load_model, **kwargs)\n",
        "\n",
        "    if not isinstance(processor, PreTrainedTokenizerBase) and hasattr(processor, 'tokenizer'):\n",
        "        tokenizer = processor.tokenizer\n",
        "        patch_getattr(processor.__class__, 'tokenizer')\n",
        "    else:\n",
        "        tokenizer = processor\n",
        "    tokenizer.model_info = model_info\n",
        "    tokenizer.model_meta = model_meta\n",
        "\n",
        "    pad_token = tokenizer.pad_token_id\n",
        "    if pad_token is None:\n",
        "        pad_token = tokenizer.eos_token_id\n",
        "    if tokenizer.eos_token_id is None:\n",
        "        tokenizer.eos_token_id = pad_token\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = pad_token\n",
        "    assert tokenizer.eos_token_id is not None\n",
        "    assert tokenizer.pad_token_id is not None\n",
        "\n",
        "    if model is not None:\n",
        "        # fix seq classification task\n",
        "        pad_token_id = model.config.pad_token_id or tokenizer.pad_token_id\n",
        "        HfConfigFactory.set_model_config_attr(model, 'pad_token_id', pad_token_id)\n",
        "        model.model_info = model_info\n",
        "        model.model_meta = model_meta\n",
        "        model.model_dir = model_dir\n",
        "\n",
        "        # generation_config\n",
        "        generation_config_path = os.path.join(model_dir, 'generation_config.json')\n",
        "        if not hasattr(model, 'generation_config') and os.path.isfile(generation_config_path):\n",
        "            model.generation_config = GenerationConfig.from_pretrained(model_dir)\n",
        "        # fix llama2 warning\n",
        "        if getattr(model, 'generation_config', None):\n",
        "            fix_do_sample_warning(model.generation_config)\n",
        "    return model, processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4gT-HrwyrVu",
        "outputId": "734e81c0-cc1b-4cd6-c619-03df83ac23bd",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run sh: `/opt/conda/envs/qwen/bin/python3.13 /opt/conda/envs/qwen/lib/python3.13/site-packages/swift/cli/sft.py --model ./Qwen-2.5-0.5B-VN --model_type qwen2_5 --train_type full --dataset ./data/proccessed_data.jsonl ./data/proccessed_data_1.jsonl ./data/proccessed_data_2.jsonl ./data/proccessed_data_3.jsonl ./data/proccessed_data_4.jsonl --torch_dtype bfloat16 --num_train_epochs 1 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --learning_rate 5e-5 --lr_scheduler_type cosine --max_grad_norm 1.0 --gradient_accumulation_steps 16 --eval_steps 500 --save_steps 500 --save_total_limit 5 --logging_steps 5 --max_length 2048 --output_dir output --system Bạn là một trợ lý AI thông minh và hữu ích. Hãy cung cấp câu trả lời chính xác, rõ ràng và dễ hiểu. --warmup_ratio 0.05 --dataloader_num_workers 4 --model_author swift --report_to wandb --model_name swift-robot`\n",
            "[INFO:swift] Successfully registered `/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
            "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
            "[INFO:swift] Loading the model using model_dir: ./Qwen-2.5-0.5B-VN\n",
            "[INFO:swift] Setting args.lazy_tokenize: False\n",
            "/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[INFO:swift] output_dir: /root/Qwen-2.5-0.5B-vn/output/v2-20250326-030102\n",
            "[INFO:swift] Global seed set to 42\n",
            "[INFO:swift] args: TrainArguments(\n",
            "_n_gpu=-1,\n",
            "acc_steps=1,\n",
            "acc_strategy=token,\n",
            "accelerator_config={'dispatch_batches': False},\n",
            "adafactor=False,\n",
            "adalora_beta1=0.85,\n",
            "adalora_beta2=0.85,\n",
            "adalora_deltaT=1,\n",
            "adalora_init_r=12,\n",
            "adalora_orth_reg_weight=0.5,\n",
            "adalora_target_r=8,\n",
            "adalora_tfinal=0,\n",
            "adalora_tinit=0,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.95,\n",
            "adam_epsilon=1e-08,\n",
            "adapter_act=gelu,\n",
            "adapter_length=128,\n",
            "adapters=[],\n",
            "add_version=True,\n",
            "attn_impl=None,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "bnb_4bit_compute_dtype=torch.bfloat16,\n",
            "bnb_4bit_quant_storage=None,\n",
            "bnb_4bit_quant_type=nf4,\n",
            "bnb_4bit_use_double_quant=True,\n",
            "boft_block_num=0,\n",
            "boft_block_size=4,\n",
            "boft_dropout=0.0,\n",
            "boft_n_butterfly_factor=1,\n",
            "check_model=True,\n",
            "ckpt_dir=None,\n",
            "columns={},\n",
            "create_checkpoint_symlink=False,\n",
            "custom_dataset_info=[],\n",
            "custom_register_path=[],\n",
            "data_seed=42,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "dataset=['./data/proccessed_data.jsonl', './data/proccessed_data_1.jsonl', './data/proccessed_data_2.jsonl', './data/proccessed_data_3.jsonl', './data/proccessed_data_4.jsonl'],\n",
            "dataset_num_proc=1,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=None,\n",
            "deepspeed=None,\n",
            "device_map=None,\n",
            "disable_tqdm=None,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "download_mode=reuse_dataset_if_exists,\n",
            "enable_cache=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_datasets=[],\n",
            "eval_datasets_args=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_generation_config=None,\n",
            "eval_limit=None,\n",
            "eval_on_start=False,\n",
            "eval_steps=500.0,\n",
            "eval_strategy=steps,\n",
            "eval_use_evalscope=False,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "external_plugins=[],\n",
            "fourier_n_frequency=2000,\n",
            "fourier_scaling=300.0,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "freeze_aligner=True,\n",
            "freeze_llm=False,\n",
            "freeze_parameters=[],\n",
            "freeze_parameters_ratio=0.0,\n",
            "freeze_vit=True,\n",
            "fsdp=,\n",
            "fsdp_config=None,\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_num=1,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "galore_cos_threshold=0.4,\n",
            "galore_gamma_proj=2,\n",
            "galore_optim_per_parameter=False,\n",
            "galore_proj_bits=4,\n",
            "galore_proj_group_size=256,\n",
            "galore_proj_quant=False,\n",
            "galore_proj_type=std,\n",
            "galore_quantization=False,\n",
            "galore_queue_size=5,\n",
            "galore_rank=128,\n",
            "galore_scale=1.0,\n",
            "galore_target_modules=None,\n",
            "galore_update_proj_gap=50,\n",
            "galore_with_embedding=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=16,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hqq_axis=None,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_args_error=False,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "init_weights=True,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "lazy_tokenize=False,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "lisa_activated_layers=0,\n",
            "lisa_step_interval=20,\n",
            "llamapro_num_groups=None,\n",
            "llamapro_num_new_blocks=4,\n",
            "load_args=False,\n",
            "load_best_model_at_end=False,\n",
            "load_data_args=False,\n",
            "load_dataset_config=None,\n",
            "local_rank=-1,\n",
            "local_repo_path=None,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/root/Qwen-2.5-0.5B-vn/output/v2-20250326-030102/runs,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "logprobs=False,\n",
            "lora_alpha=32,\n",
            "lora_bias=none,\n",
            "lora_dropout=0.05,\n",
            "lora_dtype=None,\n",
            "lora_ga_batch_size=2,\n",
            "lora_ga_direction=ArB2r,\n",
            "lora_ga_iters=2,\n",
            "lora_ga_max_length=1024,\n",
            "lora_ga_scale=stable,\n",
            "lora_ga_stable_gamma=16,\n",
            "lora_modules=[],\n",
            "lora_rank=8,\n",
            "lorap_lr_ratio=None,\n",
            "loss_scale=default,\n",
            "loss_type=None,\n",
            "lr_scheduler_kwargs=None,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_length=2048,\n",
            "max_memory={},\n",
            "max_new_tokens=64,\n",
            "max_pixels=None,\n",
            "max_steps=-1,\n",
            "metric=None,\n",
            "metric_for_best_model=loss,\n",
            "metric_warmup_step=0,\n",
            "model=./Qwen-2.5-0.5B-VN,\n",
            "model_author=['swift'],\n",
            "model_kwargs={},\n",
            "model_name=['swift-robot'],\n",
            "model_revision=None,\n",
            "model_type=qwen2_5,\n",
            "modules_to_save=[],\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "norm_bbox=None,\n",
            "num_beams=1,\n",
            "num_labels=None,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "optimizer=None,\n",
            "output_dir=/root/Qwen-2.5-0.5B-vn/output/v2-20250326-030102,\n",
            "overwrite_output_dir=False,\n",
            "packing=False,\n",
            "padding_side=right,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "problem_type=None,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "quant_bits=None,\n",
            "quant_method=None,\n",
            "ray_scope=last,\n",
            "reft_args=None,\n",
            "reft_intervention_type=LoreftIntervention,\n",
            "reft_layer_key=None,\n",
            "reft_layers=None,\n",
            "reft_rank=4,\n",
            "remove_unused_columns=True,\n",
            "repetition_penalty=None,\n",
            "report_to=['wandb'],\n",
            "response_prefix=None,\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "resume_only_model=False,\n",
            "rope_scaling=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500.0,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sequence_parallel_size=1,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "split_dataset_ratio=0.01,\n",
            "stop_words=[],\n",
            "stream=False,\n",
            "streaming=False,\n",
            "strict=False,\n",
            "swanlab_exp_name=None,\n",
            "swanlab_mode=cloud,\n",
            "swanlab_project=None,\n",
            "swanlab_token=<SWANLAB_TOKEN>,\n",
            "swanlab_workspace=None,\n",
            "system=Bạn là một trợ lý AI thông minh và hữu ích. Hãy cung cấp câu trả lời chính xác, rõ ràng và dễ hiểu.,\n",
            "target_modules=['all-linear'],\n",
            "target_regex=None,\n",
            "task_type=causal_lm,\n",
            "temperature=0.0,\n",
            "template=qwen2_5,\n",
            "template_backend=swift,\n",
            "tf32=None,\n",
            "tools_prompt=react_en,\n",
            "top_k=None,\n",
            "top_logprobs=None,\n",
            "top_p=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_dtype=torch.bfloat16,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "train_sampler_random=True,\n",
            "train_type=full,\n",
            "trainable_parameters=[],\n",
            "truncation_strategy=delete,\n",
            "tuner_backend=peft,\n",
            "use_chat_template=True,\n",
            "use_cpu=False,\n",
            "use_dora=False,\n",
            "use_galore=False,\n",
            "use_hf=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "use_rslora=False,\n",
            "use_swift_lora=False,\n",
            "val_dataset=[],\n",
            "vera_d_initial=0.1,\n",
            "vera_dropout=0.0,\n",
            "vera_projection_prng_key=0,\n",
            "vera_rank=256,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.1,\n",
            "zero_hpz_partition_size=None,\n",
            ")\n",
            "[INFO:swift] Loading the model using model_dir: ./Qwen-2.5-0.5B-VN\n",
            "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "[INFO:swift] Resizing model embedding layer from 151936 to 166473\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "[INFO:swift] New embeddings initialized with mean of original embeddings. Fine-tuning recommended.\n",
            "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
            "[INFO:swift] model_info: ModelInfo(model_type='qwen2_5', model_dir='/root/Qwen-2.5-0.5B-vn/Qwen-2.5-0.5B-VN', torch_dtype=torch.bfloat16, max_model_len=32768, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            ", task_type='causal_lm', num_labels=None)\n",
            "[INFO:swift] model.generation_config: GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"max_new_tokens\": 64,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1\n",
            "}\n",
            "\n",
            "[INFO:swift] default_system: Bạn là một trợ lý AI thông minh và hữu ích. Hãy cung cấp câu trả lời chính xác, rõ ràng và dễ hiểu.\n",
            "[INFO:swift] Start time of running main: 2025-03-26 03:01:03.661569\n",
            "Generating train split: 50006 examples [00:00, 215216.99 examples/s]\n",
            "[INFO:swift] create tmp_dir: /root/.cache/modelscope/hub/tmp/hf_datasets-8m4dpt_l\n",
            "Map:  96%|█████████████████████ | 48000/50006 [00:01<00:00, 32405.20 examples/s][INFO:swift] Traceback (most recent call last):\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 179, in batched_preprocess\n",
            "    self._check_messages(r)\n",
            "    ~~~~~~~~~~~~~~~~~~~~^^^\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 69, in _check_messages\n",
            "    raise ValueError(f'assistant_message: {assistant_message}')\n",
            "ValueError: assistant_message: {'role': 'assistant', 'content': ''}\n",
            "\n",
            "[WARNING:swift] 👆👆👆There are errors in the dataset, the data will be deleted\n",
            "Map: 100%|██████████████████████| 50006/50006 [00:01<00:00, 28709.38 examples/s]\n",
            "[INFO:swift] Dataset filtered, origin length: 50006, filtered dataset length: 50005\n",
            "Generating train split: 51092 examples [00:00, 681835.30 examples/s]\n",
            "Map: 100%|██████████████████████| 51092/51092 [00:01<00:00, 37220.61 examples/s]\n",
            "Generating train split: 90000 examples [00:00, 624420.82 examples/s]\n",
            "Map: 100%|██████████████████████| 90000/90000 [00:02<00:00, 35970.14 examples/s]\n",
            "Generating train split: 83077 examples [00:00, 513983.82 examples/s]\n",
            "Map: 100%|██████████████████████| 83077/83077 [00:02<00:00, 34451.37 examples/s]\n",
            "Generating train split: 54299 examples [00:00, 413594.27 examples/s]\n",
            "Map: 100%|██████████████████████| 54299/54299 [00:01<00:00, 33013.94 examples/s]\n",
            "[INFO:swift] train_dataset: Dataset({\n",
            "    features: ['messages'],\n",
            "    num_rows: 325191\n",
            "})\n",
            "[INFO:swift] val_dataset: Dataset({\n",
            "    features: ['messages'],\n",
            "    num_rows: 3282\n",
            "})\n",
            "[INFO:swift] The split dataset from the training set will be saved at: /root/Qwen-2.5-0.5B-vn/output/v2-20250326-030102/val_dataset.jsonl.\n",
            "Map:   1%|▏                       | 2000/325191 [00:02<05:57, 904.75 examples/s][INFO:swift] Traceback (most recent call last):\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 171, in batched_preprocess\n",
            "    row = self.preprocess(row)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/utils.py\", line 108, in preprocess\n",
            "    return self.template.encode(row)\n",
            "           ~~~~~~~~~~~~~~~~~~~~^^^^^\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 369, in encode\n",
            "    encoded = self._encode(inputs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 930, in _encode\n",
            "    raise MaxLengthError(f'Current length of row({len(input_ids)}) is larger'\n",
            "                         f' than the max_length({self.max_length}).')\n",
            "swift.llm.template.base.MaxLengthError: Current length of row(2580) is larger than the max_length(2048).\n",
            "\n",
            "[WARNING:swift] 👆👆👆There are errors in the dataset, the data will be deleted\n",
            "Map:   5%|█▏                     | 16000/325191 [00:17<05:41, 905.32 examples/s][INFO:swift] Traceback (most recent call last):\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 171, in batched_preprocess\n",
            "    row = self.preprocess(row)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/utils.py\", line 108, in preprocess\n",
            "    return self.template.encode(row)\n",
            "           ~~~~~~~~~~~~~~~~~~~~^^^^^\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 369, in encode\n",
            "    encoded = self._encode(inputs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 930, in _encode\n",
            "    raise MaxLengthError(f'Current length of row({len(input_ids)}) is larger'\n",
            "                         f' than the max_length({self.max_length}).')\n",
            "swift.llm.template.base.MaxLengthError: Current length of row(2173) is larger than the max_length(2048).\n",
            "\n",
            "[WARNING:swift] 👆👆👆There are errors in the dataset, the data will be deleted\n",
            "Map:  82%|█████████████████▎   | 268000/325191 [03:59<00:54, 1041.72 examples/s][INFO:swift] Traceback (most recent call last):\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 171, in batched_preprocess\n",
            "    row = self.preprocess(row)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/utils.py\", line 108, in preprocess\n",
            "    return self.template.encode(row)\n",
            "           ~~~~~~~~~~~~~~~~~~~~^^^^^\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 369, in encode\n",
            "    encoded = self._encode(inputs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 930, in _encode\n",
            "    raise MaxLengthError(f'Current length of row({len(input_ids)}) is larger'\n",
            "                         f' than the max_length({self.max_length}).')\n",
            "swift.llm.template.base.MaxLengthError: Current length of row(2100) is larger than the max_length(2048).\n",
            "\n",
            "[WARNING:swift] 👆👆👆There are errors in the dataset, the data will be deleted\n",
            "Map:  98%|█████████████████████▋| 320000/325191 [04:59<00:06, 849.42 examples/s][INFO:swift] Traceback (most recent call last):\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/preprocessor/core.py\", line 171, in batched_preprocess\n",
            "    row = self.preprocess(row)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/dataset/utils.py\", line 108, in preprocess\n",
            "    return self.template.encode(row)\n",
            "           ~~~~~~~~~~~~~~~~~~~~^^^^^\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 369, in encode\n",
            "    encoded = self._encode(inputs)\n",
            "  File \"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/template/base.py\", line 930, in _encode\n",
            "    raise MaxLengthError(f'Current length of row({len(input_ids)}) is larger'\n",
            "                         f' than the max_length({self.max_length}).')\n",
            "swift.llm.template.base.MaxLengthError: Current length of row(3504) is larger than the max_length(2048).\n",
            "\n",
            "[WARNING:swift] 👆👆👆There are errors in the dataset, the data will be deleted\n",
            "Map: 100%|█████████████████████| 325191/325191 [05:05<00:00, 1063.93 examples/s]\n",
            "[INFO:swift] Dataset filtered, origin length: 325191, filtered dataset length: 325187\n",
            "Map: 100%|█████████████████████████| 3282/3282 [00:03<00:00, 1063.18 examples/s]\n",
            "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 264, 5390, 323, 52897, 17847, 151645, 198, 151644, 872, 198, 154259, 12789, 129260, 129087, 128265, 128717, 128353, 128324, 29974, 79668, 129522, 9586, 128935, 128482, 128421, 128400, 13, 151645, 198, 151644, 77091, 198, 16, 13, 55986, 37915, 128444, 458, 71, 305, 88771, 128394, 133952, 128324, 29974, 79126, 131400, 128854, 128310, 128394, 69086, 128564, 132600, 129598, 62682, 132934, 1163, 124609, 128290, 220, 19, 15, 32154, 128534, 129031, 30, 220, 17, 13, 1163, 72115, 135601, 12100, 27021, 2350, 42504, 128960, 15122, 86139, 128290, 128352, 47742, 28776, 128431, 129598, 128382, 128421, 128250, 128717, 128353, 128324, 29974, 30, 220, 18, 13, 1163, 644, 128564, 138075, 129009, 129455, 128819, 11, 128300, 220, 18, 15, 14, 19, 14, 16, 24, 22, 20, 28776, 128277, 129065, 128382, 128788, 128589, 131871, 15122, 128272, 136313, 136942, 30, 151645]\n",
            "[INFO:swift] [INPUT] <|im_start|>system\n",
            "You are a useful and harmless assistant<|im_end|>\n",
            "<|im_start|>user\n",
            "Tạo ba câu hỏi về lịch sử Việt Nam để kiểm tra kiến thức đối tác.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "1. Ai là vị anh hùng dân tộc Việt Nam đã lãnh đạo nhân dân trong cuộc khởi nghĩa Hai Bà Trưng năm 40 sau Công nguyên? 2. Trận Điện Biên Phủ diễn ra vào năm nào và có ý nghĩa gì đối với lịch sử Việt Nam? 3. Trong cuộc kháng chiến chống Mỹ, ngày 30/4/1975 có sự kiện gì đặc biệt xảy ra tại Sài Gòn?<|im_end|>\n",
            "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 55986, 37915, 128444, 458, 71, 305, 88771, 128394, 133952, 128324, 29974, 79126, 131400, 128854, 128310, 128394, 69086, 128564, 132600, 129598, 62682, 132934, 1163, 124609, 128290, 220, 19, 15, 32154, 128534, 129031, 30, 220, 17, 13, 1163, 72115, 135601, 12100, 27021, 2350, 42504, 128960, 15122, 86139, 128290, 128352, 47742, 28776, 128431, 129598, 128382, 128421, 128250, 128717, 128353, 128324, 29974, 30, 220, 18, 13, 1163, 644, 128564, 138075, 129009, 129455, 128819, 11, 128300, 220, 18, 15, 14, 19, 14, 16, 24, 22, 20, 28776, 128277, 129065, 128382, 128788, 128589, 131871, 15122, 128272, 136313, 136942, 30, 151645]\n",
            "[INFO:swift] [LABELS] [-100 * 37]1. Ai là vị anh hùng dân tộc Việt Nam đã lãnh đạo nhân dân trong cuộc khởi nghĩa Hai Bà Trưng năm 40 sau Công nguyên? 2. Trận Điện Biên Phủ diễn ra vào năm nào và có ý nghĩa gì đối với lịch sử Việt Nam? 3. Trong cuộc kháng chiến chống Mỹ, ngày 30/4/1975 có sự kiện gì đặc biệt xảy ra tại Sài Gòn?<|im_end|>\n",
            "Map: 100%|█████████████████████| 325187/325187 [00:41<00:00, 7781.48 examples/s]\n",
            "[INFO:swift] Dataset Token Length: 182.774920±128.457205, min=27.000000, max=1914.000000, size=325187\n",
            "Map: 100%|█████████████████████████| 3282/3282 [00:00<00:00, 7765.29 examples/s]\n",
            "[INFO:swift] Dataset Token Length: 183.287934±133.510944, min=33.000000, max=1676.000000, size=3282\n",
            "[INFO:swift] The TrainArguments will be saved in: /root/Qwen-2.5-0.5B-vn/output/v2-20250326-030102/args.json\n",
            "[INFO:swift] model: Qwen2ForCausalLM(\n",
            "  (model): Qwen2Model(\n",
            "    (embed_tokens): Embedding(166473, 896)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "    (rotary_emb): Qwen2RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=896, out_features=166473, bias=False)\n",
            ")\n",
            "[INFO:swift] model_parameter_info: Qwen2ForCausalLM: 507.0579M Params (507.0579M Trainable [100.0000%]), 0.0000M Buffers.\n",
            "/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/trainers/mixin.py:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/cli/sft.py\"\u001b[0m, line \u001b[35m10\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    \u001b[31msft_main\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/train/sft.py\"\u001b[0m, line \u001b[35m265\u001b[0m, in \u001b[35msft_main\u001b[0m\n",
            "    return \u001b[31mSwiftSft(args).main\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/base.py\"\u001b[0m, line \u001b[35m47\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    result = self.run()\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/llm/train/sft.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35mrun\u001b[0m\n",
            "    trainer = trainer_cls(\n",
            "        model=self.model,\n",
            "    ...<6 lines>...\n",
            "        **self._get_trainer_kwargs(),\n",
            "    )\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/trainers/trainers.py\"\u001b[0m, line \u001b[35m85\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    \u001b[31msuper().__init__\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/swift/trainers/mixin.py\"\u001b[0m, line \u001b[35m81\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    \u001b[31msuper().__init__\u001b[0m\u001b[1;31m(\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
            "        \u001b[1;31mmodel=model,\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
            "    ...<9 lines>...\n",
            "        \u001b[1;31mpreprocess_logits_for_metrics=preprocess_logits_for_metrics,\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        \u001b[1;31m**kwargs)\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/utils/deprecation.py\"\u001b[0m, line \u001b[35m172\u001b[0m, in \u001b[35mwrapped_func\u001b[0m\n",
            "    return func(*args, **kwargs)\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/trainer_seq2seq.py\"\u001b[0m, line \u001b[35m73\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    \u001b[31msuper().__init__\u001b[0m\u001b[1;31m(\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
            "        \u001b[1;31mmodel=model,\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
            "    ...<10 lines>...\n",
            "        \u001b[1;31mpreprocess_logits_for_metrics=preprocess_logits_for_metrics,\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "    \u001b[1;31m)\u001b[0m\n",
            "    \u001b[1;31m^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/utils/deprecation.py\"\u001b[0m, line \u001b[35m172\u001b[0m, in \u001b[35mwrapped_func\u001b[0m\n",
            "    return func(*args, **kwargs)\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m679\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    self.callback_handler = \u001b[31mCallbackHandler\u001b[0m\u001b[1;31m(\u001b[0m\n",
            "                            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
            "        \u001b[1;31mcallbacks, self.model, self.processing_class, self.optimizer, self.lr_scheduler\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "    \u001b[1;31m)\u001b[0m\n",
            "    \u001b[1;31m^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/trainer_callback.py\"\u001b[0m, line \u001b[35m449\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    \u001b[31mself.add_callback\u001b[0m\u001b[1;31m(cb)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/trainer_callback.py\"\u001b[0m, line \u001b[35m466\u001b[0m, in \u001b[35madd_callback\u001b[0m\n",
            "    cb = \u001b[31mcallback\u001b[0m\u001b[1;31m()\u001b[0m if isinstance(callback, type) else callback\n",
            "         \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/conda/envs/qwen/lib/python3.13/site-packages/transformers/integrations/integration_utils.py\"\u001b[0m, line \u001b[35m787\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    raise RuntimeError(\"WandbCallback requires wandb to be installed. Run `pip install wandb`.\")\n",
            "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mWandbCallback requires wandb to be installed. Run `pip install wandb`.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 20GiB\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!swift sft \\\n",
        "    --model './Qwen-2.5-0.5B-VN' \\\n",
        "    --model_type 'qwen2_5' \\\n",
        "    --train_type full \\\n",
        "    --dataset \"./data/proccessed_data.jsonl\" \\\n",
        "    './data/proccessed_data_1.jsonl' \\\n",
        "    './data/proccessed_data_2.jsonl' \\\n",
        "    './data/proccessed_data_3.jsonl' \\\n",
        "    './data/proccessed_data_4.jsonl' \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --eval_steps 500 \\\n",
        "    --save_steps 500 \\\n",
        "    --save_total_limit 5 \\\n",
        "    --logging_steps 5 \\\n",
        "    --max_length 2048 \\\n",
        "    --output_dir output \\\n",
        "    --system 'Bạn là một trợ lý AI thông minh và hữu ích. Hãy cung cấp câu trả lời chính xác, rõ ràng và dễ hiểu.' \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --dataloader_num_workers 4 \\\n",
        "    --model_author swift \\\n",
        "    --report_to wandb \\\n",
        "    --model_name swift-robot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "CUDA_VISIBLE_DEVICES=0 swift sft \\\n",
        "    --model '/root/Qwen-2.5-0.5B-vn/Qwen-2.5-0.5B-VN' \\\n",
        "    --model_type 'qwen2_5' \\\n",
        "    --train_type full \\\n",
        "    --dataset \"./data/proccessed_data.jsonl\" \\\n",
        "    './data/proccessed_data_1.jsonl' \\\n",
        "    './data/proccessed_data_2.jsonl' \\\n",
        "    './data/proccessed_data_3.jsonl' \\\n",
        "    './data/proccessed_data_4.jsonl' \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --eval_steps 500 \\\n",
        "    --save_steps 500 \\\n",
        "    --save_total_limit 5 \\\n",
        "    --logging_steps 5 \\\n",
        "    --max_length 2048 \\\n",
        "    --output_dir output \\\n",
        "    --system \"Bạn là một trợ lý AI thông minh và hữu ích. Hãy cung cấp câu trả lời chính xác, rõ ràng và dễ hiểu.\" \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --dataloader_num_workers 4 \\\n",
        "    --model_author swift \\\n",
        "    --report_to wandb \\\n",
        "    --model_name swift-robot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX8JXMHpYXtz",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qwen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
